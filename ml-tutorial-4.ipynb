{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, metrics, datasets, svm, preprocessing\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie do SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykorzystamy zestaw danych Iris - zawiera on dane dla próbek z trzech gatunków irysów zawierające 4 pomiary\n",
    "# dla każdego - długość i szerokość płatka oraz działki kielicha\n",
    "iris = datasets.load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do analizy bierzemy pierwsze dwa mierzone parametry, tzn. długość i szerokość działki kielicha\n",
    "X, y = X_iris[:, :2], y_iris\n",
    "y = preprocessing.label_binarize(y, classes=[0, 1, 2])  # aby nie było problemów przy wyznaczaniu ROC\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Przygotowujemy zbiór testowy\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "# Dla klasyfikatora użytego w dalszej części wskazana jest normalizacja danych, co też właśnie uczynimy\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizujemy nasz zbiór danych\n",
    "colors = ['red', 'greenyellow', 'blue']\n",
    "for i in range(len(colors)):\n",
    "    px = X_train[:, 0][y_train[:,i] == 1]\n",
    "    py = X_train[:, 1][y_train[:,i] == 1]\n",
    "    plt.scatter(px, py, c=colors[i])\n",
    "\n",
    "plt.legend(iris.target_names)\n",
    "plt.xlabel('Długość')\n",
    "plt.ylabel('Szerokość')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikator liniowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, oddzielenie za pomocą prostej czerwonych punktów od pozostałych nie powinno sprawiać problemów, czego nie można powiedzieć o oddzieleniu pozostałych klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzymy i trenujemy klasyfikator SGD - Stochastic Gradient Descent\n",
    "clf = OneVsRestClassifier(linear_model.SGDClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Otrzymane współczynniki\n",
    "print (clf.coef_)\n",
    "print (clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzymy funkcję do wizualizacji wyników (przyda się później)\n",
    "def draw_results(clf):\n",
    "    # Nanosimy na wykres punkty z całego zbioru\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        idx = np.where(y[:,i] == 1)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i])\n",
    "    plt.axis('tight')  # ta opcja zapewnia \"nierozjechanie\" się wykresu po dodaniu prostych\n",
    "\n",
    "    # Dodajemy otrzymane z klasyfikacji linie proste\n",
    "    xmin, xmax = plt.xlim()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    coefficients = clf.coef_\n",
    "    intercept = clf.intercept_\n",
    "\n",
    "    def plot_line(index, color):\n",
    "        def line(x0):\n",
    "            return (-(x0 * coefficients[index, 0]) - intercept[index]) / coefficients[index, 1]\n",
    "\n",
    "        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "                 ls=\"--\", color=color)\n",
    "\n",
    "\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        plot_line(i, color)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizujemy wyniki\n",
    "draw_results(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdźmy dokładność\n",
    "y_predicted = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikator SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tworzymy liniowy klasyfikator SVM\n",
    "linear_svc = OneVsRestClassifier(svm.SVC(kernel=\"linear\")).fit(X_train, y_train)\n",
    "draw_results(linear_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy dokładność\n",
    "y_predicted = linear_svc.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trik kernelowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stosowanie wyłącznie liniowych funkcji jest dość ograniczające. W sytuacjach, gdy takie podejście jest niewystarczające z pomocą przychodzi tzw. trik kernelowy - matematycznie polega on na transformacji danych do wyżej wymiarowej przestrzeni, w której można znaleźć hiperpłaszczyznę separującą dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zawczasu przygotujmy sobie funkcję rysującą nieliniowe kontury\n",
    "def draw_nonlinear_results(clf, h=0.05):\n",
    "    # Nanosimy na wykres punkty z całego zbioru\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        idx = np.where(y[:,i] == 1)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i])\n",
    "    plt.axis('tight')  # ta opcja zapewnia \"nierozjechanie\" się wykresu po dodaniu prostych\n",
    "\n",
    "    # Dodajemy otrzymane z klasyfikacji linie proste\n",
    "    xmin, xmax = plt.xlim()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    \n",
    "    # Tworzymy siatkę do narysowania konturów\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, h),\n",
    "                         np.arange(ymin, ymax, h))\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    index_list = [np.where(i == np.amax(i)) for i in Z]\n",
    "    Z = list(map(lambda x: list(x[0]), index_list))\n",
    "    Z = np.array([x[0] if x else 3 for x in Z])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, colors=colors, linestyles=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spróbujmy zastosować wielomian\n",
    "degree = 3\n",
    "gamma = 0.9\n",
    "polynomial_clf = OneVsRestClassifier(svm.SVC(kernel=\"poly\", degree=degree, gamma=gamma, probability=True))\n",
    "polynomial_clf.fit(X_train, y_train)\n",
    "\n",
    "draw_nonlinear_results(polynomial_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy dokładność\n",
    "y_predicted = polynomial_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innym możliwym wyborem jest kernel RBF\n",
    "rbf_clf = OneVsRestClassifier(svm.SVC(kernel=\"rbf\", gamma=gamma, probability=True))\n",
    "rbf_clf.fit(X_train, y_train)\n",
    "draw_nonlinear_results(rbf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy dokładność\n",
    "y_predicted = rbf_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badanie jakości klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "def determine_roc(clf, title=\"\"):\n",
    "    y_score = clf.decision_function(X_test)\n",
    "    \n",
    "    # wyznaczamy tpr i fpr dla wszystkich klas osobno\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(0, n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # wartości micro\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # wartości zbiorcze\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))    \n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # rysowanie wykresu\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label='micro-uśredniona krzywa ROC (pole = {0:0.2f})'\n",
    "            ''.format(roc_auc[\"micro\"]),\n",
    "            color=\"deeppink\", linestyle=\":\", linewidth=4)\n",
    "    \n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label='macro-uśredniona krzywa ROC (pole = {0:0.2f})'\n",
    "            ''.format(roc_auc[\"macro\"]),\n",
    "            color=\"navy\", linestyle=\":\", linewidth=4)\n",
    "    \n",
    "    for i, color in zip(range(0, n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "                label=\"ROC dla klasy {} (pole = {:0.2f})\"\n",
    "                \"\".format(i, roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0,1], [0,1], \"k--\", linewidth=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False positive\")\n",
    "    plt.ylabel(\"True positive\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(title)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC dla naszych klasyfikatorów\n",
    "determine_roc(clf, \"Klasyfikator liniowy\")\n",
    "determine_roc(linear_svc, \"Liniowy SVM\")\n",
    "determine_roc(polynomial_clf, \"Wielomian\")\n",
    "determine_roc(rbf_clf, \"RBF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-recall\n",
    "def determine_precision_recall(clf, title):\n",
    "    y_score = clf.decision_function(X_test)\n",
    "    \n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                           y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "        \n",
    "    # micro średnia\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "                                                                   y_score.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n",
    "                                                        average=\"micro\")\n",
    "    \n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "    lines.append(l)\n",
    "    labels.append(\"Krzywe Iso-F1\")\n",
    "    l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color=\"gold\", linewidth=2)\n",
    "    lines.append(l)\n",
    "    labels.append(\"Micro-średni precision-recall (pole = {:0.2f})\"\n",
    "                 \"\".format(average_precision[\"micro\"]))\n",
    "    \n",
    "    for i, color in zip(range(0, n_classes), colors):\n",
    "        l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall dla klasy {} (pole = {:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.legend(lines, labels, loc=(0, -0.38), prop=dict(size=14))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-recall dla naszych klasyfikatorów\n",
    "determine_precision_recall(clf, \"Klasyfikator liniowy\")\n",
    "determine_precision_recall(linear_svc, \"Liniowy SVM\")\n",
    "determine_precision_recall(polynomial_clf, \"Wielomian\")\n",
    "determine_precision_recall(rbf_clf, \"RBF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Działanie klasyfikatora SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy tutaj zbioru WBC (wisconsin breast cancer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#ładownie danych\n",
    "wbc = datasets.load_breast_cancer()\n",
    "x_wbc, y_wbc = wbc.data[:300], wbc.target[:300]\n",
    "\n",
    "y_wbc = preprocessing.label_binarize(y_wbc, classes=[0, 1, 2])  # aby nie było problemów przy wyznaczaniu ROC\n",
    "y_wbc = y_wbc[:, :2]\n",
    "n_classes = y_wbc.shape[1]\n",
    "\n",
    "# Przygotowujemy zbiór testowy\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_wbc, y_wbc, test_size=test_size)\n",
    "\n",
    "# Dla klasyfikatora użytego w dalszej części wskazana jest normalizacja danych, co też właśnie uczynimy\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "x_wbc = scaler.transform(x_wbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# użyjemy trzech różnych kerneli: linear, rbf i poly\n",
    "\n",
    "#linear\n",
    "linear_svc = OneVsRestClassifier(svm.SVC(kernel=\"linear\")).fit(X_train, y_train)\n",
    "y_predicted_linear = linear_svc.predict(X_test)\n",
    "\n",
    "#wielomian\n",
    "degree = 3\n",
    "gamma = 0.9\n",
    "polynomial_clf = OneVsRestClassifier(svm.SVC(kernel=\"poly\", degree=degree, gamma=gamma, probability=True))\n",
    "polynomial_clf.fit(X_train, y_train)\n",
    "y_predicted_poly = polynomial_clf.predict(X_test)\n",
    "\n",
    "#rbf\n",
    "rbf_clf = OneVsRestClassifier(svm.SVC(kernel=\"rbf\", gamma=gamma, probability=True))\n",
    "rbf_clf.fit(X_train, y_train)\n",
    "y_predicted_rbf = rbf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdzamy najpierw dokładność wszystkich trzech kerneli\n",
    "\n",
    "print(\"Dokładność dla linear: \")\n",
    "print(metrics.accuracy_score(y_test, y_predicted_linear))\n",
    "print(\"Dokładność dla poly: \")\n",
    "print(metrics.accuracy_score(y_test, y_predicted_poly))\n",
    "print(\"Dokładność dla rbf: \")\n",
    "print(metrics.accuracy_score(y_test, y_predicted_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dokonamy teraz walidacji krzyżowej:\n",
    "\n",
    "# ROC wraz z polem pod wykresem\n",
    "determine_roc(linear_svc, \"Liniowy SVM\")\n",
    "determine_roc(polynomial_clf, \"Wielomian\")\n",
    "determine_roc(rbf_clf, \"RBF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PR wraz z miarą F-1\n",
    "determine_precision_recall(linear_svc, \"Liniowy SVM\")\n",
    "determine_precision_recall(polynomial_clf, \"Wielomian\")\n",
    "determine_precision_recall(rbf_clf, \"RBF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PCA - Principal Component Analysis\n",
    "\n",
    "PCA, czyli Analiza Składowych Głównych, służy do redukcji liczby zmiennych opisujących zjawiska, czy do odkrycia prawidłowości między zmiennymi.\n",
    "\n",
    "Polega ona na wyznaczeniu składowych będących kombinacją liniową badanych zmiennych. Dokładna analiza składowych głównych umożliwia wskazanie tych zmiennych początkowych, które mają duży wpływ na wygląd poszczególnych składowych głównych czyli tych, które tworzą grupę jednorodną. Składowa główna (u której wariancja jest zmaksymalizowana) jest wówczas reprezentantem tej grupy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_PCA_chart(data, labels, colors):\n",
    "    #Użyj PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data)\n",
    "    data = pca.transform(data)\n",
    "\n",
    "    #Narysuj wykres\n",
    "    c = np.array([colors[x] for x in labels])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=c)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Używamy zbioru danych \"breast cancer wisconsin\"\n",
    "dataset = datasets.load_breast_cancer()\n",
    "x_wbc, y_wbc = dataset.data[:300], dataset.target[:300]\n",
    "\n",
    "# Przygotowujemy zbiór testowy\n",
    "test_size = 0.25\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_wbc, y_wbc, test_size=test_size)\n",
    "\n",
    "colors = { 0: \"red\", 1: \"blue\" }\n",
    "\n",
    "# Rysujemy wykres\n",
    "draw_PCA_chart(X_test, Y_test, colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na zbiorze MNIST dokonaj klasyfikacji metodą SVM dla dwóch kerneli (POLY oraz RBF), a także\n",
    "metodą k-NN sla k-1, 3 i 5.\n",
    "Podglądnij otrzymane wyniki przy pomocy PCA (skozystaj z kodu z poprzednich punktów)\n",
    "\n",
    "Następnie skomentuj otrzymane wyniki\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "# pobranie danych\n",
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./mnist_784\", cache=True)\n",
    "\n",
    "elements = 1000\n",
    "test_size = 0.25\n",
    "\n",
    "m_data = mnist.data[:elements]\n",
    "m_target = mnist.target[:elements]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(m_data, m_target, test_size=test_size)\n",
    "\n",
    "# normalizacja\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# wykorzystaj klasyfikatory z punktu 2.\n",
    "# np.:\n",
    "# class_clf = OneVsRestClassifier(svm.SVC(kernel=\"linear\"))\n",
    "\n",
    "# knn classifier - przypomnienie\n",
    "# knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Wyniki zaprezentuj używając metody draw_PCA_chart\n",
    "# możesz wykorzystać te kolory do wizualizacji:\n",
    "color_map = {\n",
    "    \"0\": \"cyan\",\n",
    "    \"1\": \"orange\",\n",
    "    \"2\": \"green\",\n",
    "    \"3\": \"red\",\n",
    "    \"4\": \"darkred\",\n",
    "    \"5\": \"purple\",\n",
    "    \"6\": \"gray\",\n",
    "    \"7\": \"blue\",\n",
    "    \"8\": \"greenyellow\",\n",
    "    \"9\": \"brown\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komentarz:\n",
    "<uzupełnij>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zrób to samo, lecz tym razem zredukuj wymiarowość danych do 40 wymiarów metodą PCA.\n",
    "Czy wyniki uległy poprawie? Jeśli tak, to dlaczego? Skomentuj otrzymane wyniki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "# redukcja wymiarów:\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komentarz:\n",
    "<uzupełnij>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Opracowano na podstawie\n",
    "* https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "* https://adataanalyst.com/scikit-learn/linear-classification-method/\n",
    "* https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html\n",
    "* https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html\n",
    "* https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "* https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e\n",
    "* https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d\n",
    "* https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "* https://www.statystyka.az.pl/analiza-skladowych-glownych-pca.php\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}